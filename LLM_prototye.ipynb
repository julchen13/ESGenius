{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00IwhEm_2cJq",
        "outputId": "9fe7604b-4fb9-44f6-fb16-83661cd74c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 7.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 57.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 494.0/494.0 kB 39.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 303.4/303.4 kB 28.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/20.0 MB 114.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 94.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.1/54.1 MB 45.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 323.1/323.1 kB 30.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.2/95.2 kB 10.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 9.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 133.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 7.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 2.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 128.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 101.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 59.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 4.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 44.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 19.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 5.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 106.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 6.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.0/264.0 kB 21.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 5.5 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install --upgrade haystack-ai anthropic-haystack pypdf PyMuPDF reportlab Pillow typing jinja2 --upgrade gradio --upgrade reportlab langdetect  \"sentence-transformers>=2.2.0\" --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "xZwXQsYx4UJO",
        "outputId": "5ab16d40-48d3-4c70-e9d3-8d1aa63d717b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/dropdown.py:227: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: manual input or set allow_custom_value=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8ec436f87dbde07e63.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8ec436f87dbde07e63.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8ec436f87dbde07e63.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators.utils import print_streaming_chunk\n",
        "from haystack.utils import Secret\n",
        "from haystack_integrations.components.generators.anthropic import AnthropicChatGenerator\n",
        "from haystack.components.generators import HuggingFaceAPIGenerator, OpenAIGenerator\n",
        "from typing import List, Optional, Dict\n",
        "from haystack.dataclasses import ChatMessage, Document\n",
        "from haystack import component\n",
        "from jinja2 import Template\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "# Bestehende Komponenten\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.converters import PyPDFToDocument\n",
        "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
        "from haystack.components.joiners import DocumentJoiner\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
        "from haystack.dataclasses import ChatMessage\n",
        "\n",
        "# for pdf\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
        "from reportlab.lib.units import mm\n",
        "from reportlab.platypus import PageTemplate, Frame\n",
        "from reportlab.pdfgen import canvas\n",
        "from PIL import Image as PILImage\n",
        "from reportlab.lib.colors import HexColor\n",
        "\n",
        "from langdetect import detect\n",
        "\n",
        "# detect language of uploaded documents\n",
        "# to help the LLM answer in english if questions and documents are in english\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return 'en'  # Default to English if detection fails\n",
        "\n",
        "# Globale variables\n",
        "global global_model_name\n",
        "global global_output_dir\n",
        "global_model_name = \"Nicht spezifiziert\"\n",
        "global_output_dir = None\n",
        "\n",
        "\n",
        "# some predefined questions to be presented in a dropdown menu\n",
        "predefined_questions = [\n",
        "    \"Wie hoch ist die Solvenzquote?\",\n",
        "    \"manual input\"\n",
        "]\n",
        "\n",
        "# read api key for each selected model from local text file\n",
        "def read_api_key(model_name):\n",
        "    key_file_map = {\n",
        "        \"claude\": \"/content/anthropic_api_key.txt\",\n",
        "        \"gpt\": \"/content/openai_api_key.txt\",\n",
        "        \"default\": \"/content/huggingface_api_key.txt\"\n",
        "    }\n",
        "\n",
        "    if \"claude\" in model_name.lower():\n",
        "        key_file = key_file_map[\"claude\"]\n",
        "    elif \"gpt\" in model_name.lower():\n",
        "        key_file = key_file_map[\"gpt\"]\n",
        "    else:\n",
        "        key_file = key_file_map[\"default\"]\n",
        "\n",
        "    try:\n",
        "        with open(key_file, 'r') as file:\n",
        "            return file.read().strip()\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"API key file not found: {key_file}\")\n",
        "\n",
        "# initializing inmemory document store\n",
        "# store documents in local memory only\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "# function to clear document store when documents are deleted from interface\n",
        "def clear_document_store():\n",
        "    global document_store\n",
        "    document_store = InMemoryDocumentStore()\n",
        "    print(\"Document store cleared. A new empty store has been created.\")\n",
        "\n",
        "# batch embedding algorithms - needed to process large documents that won't fit into the models token window\n",
        "# helps to reduce RAM and GPU requirements\n",
        "@component\n",
        "# divides a large list of documents into smaller batches (default: 8 documents per batch)\n",
        "# process each batch separately through the Sentence transformer embedder and merges the results\n",
        "class BatchSentenceTransformersDocumentEmbedder:\n",
        "    def __init__(self, batch_size: int = 8):\n",
        "        self.embedder = SentenceTransformersDocumentEmbedder()\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    @component.output_types(documents=List[Document])\n",
        "    def run(self, documents: List[Document]):\n",
        "        embeddings = []\n",
        "        self.embedder.warm_up()\n",
        "        for i in range(0, len(documents), self.batch_size):\n",
        "            batch = documents[i:i+self.batch_size]\n",
        "            batch_embeddings = self.embedder.run(documents=batch)['documents']\n",
        "            embeddings.extend(batch_embeddings)\n",
        "        return {\"documents\": embeddings}\n",
        "\n",
        "@component\n",
        "# divides long texts into chunks (default: 4000 characters)\n",
        "# generates separate embeddings for each chunk\n",
        "# calculates average embeddings to retrieve a single representation of the complete text\n",
        "class BatchSentenceTransformersTextEmbedder:\n",
        "    def __init__(self):\n",
        "        self.embedder = SentenceTransformersTextEmbedder()\n",
        "\n",
        "    @component.output_types(embedding=List[float])\n",
        "    def run(self, text: str):\n",
        "        chunks = [text[i:i+4000] for i in range(0, len(text), 4000)]\n",
        "        embeddings = []\n",
        "        self.embedder.warm_up()\n",
        "        for chunk in chunks:\n",
        "            embedding = self.embedder.run(text=chunk)['embedding']\n",
        "            embeddings.append(embedding)\n",
        "        avg_embedding = np.mean(embeddings, axis=0).tolist()\n",
        "        return {\"embedding\": avg_embedding}\n",
        "\n",
        "# template handling: original template may be updated by user\n",
        "# make sure that important parts of template (protected placeholders) are not changed and the original template may be restored\n",
        "# Template Definition\n",
        "ORIGINAL_TEMPLATE = \"\"\"\n",
        "Role: You are acting as a Supervisory ESG Compliance Advisor.\n",
        "Objective: Your task is to evaluate whether a financial fund qualifies as an ESG (Environmental, Social, and Governance) fund under the Sustainable Finance Disclosure Regulation (SFDR), and to determine whether it falls under Article 8 or Article 9 classification.\n",
        "\n",
        "Instructions:\n",
        "1. Stick strictly to the facts presented in the document.\n",
        "2. If specific information is not available, clearly state: “I cannot find this information.”\n",
        "3. For each piece of information, include the paragraph number and page number where it was found in the document.\n",
        "4. Ensure the output is easy to read, visually separated, and traceable to the source document.\n",
        "\n",
        "Contexts:\n",
        "{% for doc_name, documents in all_documents.items() %}\n",
        "Document: {{ doc_name }}\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{ question }}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Global variable to store the current template\n",
        "current_template = ORIGINAL_TEMPLATE\n",
        "\n",
        "# Define the placeholders that should not be changed, with their required occurrences\n",
        "PROTECTED_PLACEHOLDERS: Dict[str, int] = {\n",
        "    \"{% for doc_name, documents in all_documents.items() %}\": 1,\n",
        "    \"{% for document in documents %}\": 1,\n",
        "    \"{% endfor %}\": 2,  # This placeholder should appear twice\n",
        "    \"{{ doc_name }}\": 1,\n",
        "    \"{{ document.content }}\": 1,\n",
        "    \"{{ question }}\": 1\n",
        "}\n",
        "\n",
        "def validate_template(template: str) -> Tuple[bool, str]:\n",
        "    placeholder_counts = {placeholder: 0 for placeholder in PROTECTED_PLACEHOLDERS}\n",
        "\n",
        "    for placeholder, required_count in PROTECTED_PLACEHOLDERS.items():\n",
        "        actual_count = template.count(placeholder)\n",
        "        placeholder_counts[placeholder] = actual_count\n",
        "\n",
        "        if actual_count < required_count:\n",
        "            return False, f\"Protected placeholder missing or insufficient: '{placeholder}'. Expected {required_count}, found {actual_count}.\"\n",
        "        elif actual_count > required_count:\n",
        "            return False, f\"Too many occurrences of protected placeholder: '{placeholder}'. Expected {required_count}, found {actual_count}.\"\n",
        "\n",
        "    return True, \"Template is valid\"\n",
        "\n",
        "def update_template(new_template: str) -> str:\n",
        "    global current_template\n",
        "    is_valid, message = validate_template(new_template)\n",
        "    if is_valid:\n",
        "        current_template = new_template\n",
        "        return \"Template updated successfully\"\n",
        "    else:\n",
        "        return f\"Template update failed: {message}\"\n",
        "\n",
        "def update_template_with_validation(new_template: str) -> Tuple[str, str]:\n",
        "    result = update_template(new_template)\n",
        "    if \"successfully\" in result:\n",
        "        return result, new_template\n",
        "    else:\n",
        "        return result, current_template\n",
        "\n",
        "def reset_template() -> Tuple[str, str]:\n",
        "    global current_template\n",
        "    current_template = ORIGINAL_TEMPLATE\n",
        "    return ORIGINAL_TEMPLATE, \"Template reset to original\"\n",
        "\n",
        "# convert the prompt string into a ChatMessage to be used by LLM\n",
        "@component\n",
        "class PromptToChatMessage:\n",
        "    @component.output_types(messages=List[ChatMessage])\n",
        "    def run(self, prompt: str):\n",
        "        return {\"messages\": [ChatMessage.from_user(prompt)]}\n",
        "\n",
        "# Modify the CustomPromptBuilder to use the current_template\n",
        "@component\n",
        "class CustomPromptBuilder:\n",
        "    @component.output_types(prompt=str)\n",
        "    def run(self, question: str, documents: List[Document], all_documents: Optional[Dict[str, List[Document]]] = None):\n",
        "        if all_documents is None:\n",
        "            all_documents = {\"Default\": documents}\n",
        "        template = Template(current_template)\n",
        "        context = template.render(all_documents=all_documents, question=question)\n",
        "        return {\"prompt\": context}\n",
        "# end template handling\n",
        "\n",
        "# functions to process pdfs\n",
        "# change pdfs into a format to be processed by LLMs\n",
        "def process_pdfs(pdf_files, split_by, split_length, split_overlap):\n",
        "    global document_store\n",
        "    # clear the document store before processing new documents to avoid including old documents into the analysis\n",
        "    clear_document_store()\n",
        "\n",
        "    all_documents = {}\n",
        "    total_docs = 0\n",
        "    for pdf_file in pdf_files:\n",
        "        try:\n",
        "            preprocessing_pipeline = Pipeline()\n",
        "            preprocessing_pipeline.add_component(instance=PyPDFToDocument(), name=\"pdf_converter\") # converts pdf to text\n",
        "            preprocessing_pipeline.add_component(instance=DocumentJoiner(), name=\"document_joiner\") # joins documents together\n",
        "            preprocessing_pipeline.add_component(instance=DocumentCleaner(), name=\"document_cleaner\") # cleans text before processing\n",
        "            preprocessing_pipeline.add_component(instance=DocumentSplitter(split_by=split_by, split_length=split_length, split_overlap=split_overlap), name=\"document_splitter\") # splits large documents into smaller chunks\n",
        "            preprocessing_pipeline.add_component(instance=BatchSentenceTransformersDocumentEmbedder(), name=\"document_embedder\")\n",
        "\n",
        "            preprocessing_pipeline.connect(\"pdf_converter\", \"document_joiner\")\n",
        "            preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
        "            preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
        "            preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
        "\n",
        "            print(f\"Processing file: {pdf_file.name}\")\n",
        "            result = preprocessing_pipeline.run({\"pdf_converter\": {\"sources\": [pdf_file.name]}})\n",
        "            print(f\"Pipeline result keys: {result.keys()}\")\n",
        "\n",
        "            if 'document_embedder' in result and 'documents' in result['document_embedder']:\n",
        "                docs = result['document_embedder']['documents']\n",
        "                all_documents[pdf_file.name] = docs\n",
        "                print(f\"Processed {len(docs)} documents from {pdf_file.name}\")\n",
        "\n",
        "                # Store documents in the document store\n",
        "                for doc in docs:\n",
        "                    doc.meta['source'] = pdf_file.name # add meta data about data source to each document\n",
        "                document_store.write_documents(docs) # save documents in document store\n",
        "                total_docs += len(docs)\n",
        "            else:\n",
        "                print(f\"No documents found in the result for {pdf_file.name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file.name}: {str(e)}\")\n",
        "\n",
        "    print(f\"Total documents processed and stored: {total_docs}\")\n",
        "    return f\"{len(pdf_files)} PDFs processed. Total of {total_docs} documents stored in the Document Store. Parameters: split_by={split_by}, split_length={split_length}, split_overlap={split_overlap}\"\n",
        "\n",
        "# pipelines for different llm model types: take into account different api specifications\n",
        "def setup_claude_pipeline(model_name):\n",
        "    pipe = Pipeline()\n",
        "    pipe.add_component(\"embedder\", BatchSentenceTransformersTextEmbedder())\n",
        "    pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=10))\n",
        "    pipe.add_component(\"prompt_builder\", CustomPromptBuilder())\n",
        "    pipe.add_component(\"prompt_to_chat\", PromptToChatMessage())\n",
        "\n",
        "    # take api key provided by local text file\n",
        "    api_key = read_api_key(model_name)\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
        "    pipe.add_component(\"llm\", AnthropicChatGenerator(\n",
        "        api_key=Secret.from_token(api_key),\n",
        "        streaming_callback=print_streaming_chunk,\n",
        "        model=model_name\n",
        "    ))\n",
        "\n",
        "    pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
        "    pipe.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "    pipe.connect(\"prompt_builder.prompt\", \"prompt_to_chat.prompt\")\n",
        "    pipe.connect(\"prompt_to_chat.messages\", \"llm.messages\")\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def setup_openai_pipeline(model_name):\n",
        "    pipe = Pipeline()\n",
        "    pipe.add_component(\"embedder\", BatchSentenceTransformersTextEmbedder())\n",
        "    pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=10))\n",
        "    pipe.add_component(\"prompt_builder\", CustomPromptBuilder())\n",
        "\n",
        "    # take api key provided by local text file\n",
        "    api_key = read_api_key(model_name)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    pipe.add_component(\"llm\", OpenAIGenerator(\n",
        "        api_key=Secret.from_token(api_key),\n",
        "        model=model_name,\n",
        "        streaming_callback=print_streaming_chunk\n",
        "    ))\n",
        "\n",
        "    pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
        "    pipe.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "    pipe.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def setup_other_pipeline(model_name):\n",
        "    pipe = Pipeline()\n",
        "    pipe.add_component(\"embedder\", BatchSentenceTransformersTextEmbedder())\n",
        "    pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=10))\n",
        "    pipe.add_component(\"prompt_builder\", CustomPromptBuilder())\n",
        "\n",
        "    # read api key provided by local text file\n",
        "    api_key = read_api_key(model_name)\n",
        "    os.environ[\"HUGGINGFACE_API_TOKEN\"] = api_key\n",
        "    pipe.add_component(\"llm\", HuggingFaceAPIGenerator(\n",
        "        api_type=\"serverless_inference_api\",\n",
        "        api_params={\"model\": model_name},\n",
        "        token=Secret.from_token(api_key)\n",
        "    ))\n",
        "\n",
        "    pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
        "    pipe.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "    pipe.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# controls the length of the response for the Gradio interface\n",
        "def truncate_text(text: str, max_tokens: int) -> str:\n",
        "    \"\"\"\n",
        "    Truncate the text to approximately max_tokens.\n",
        "    This is a simple approximation and might not be exact.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) <= max_tokens:\n",
        "        return text\n",
        "    return ' '.join(words[:max_tokens]) + '...'\n",
        "\n",
        "# process of taking a question and generating an answer by LLM\n",
        "def get_question_and_answer(model, predefined, manual, history, output_dir):\n",
        "    global global_model_name, global_output_dir\n",
        "    global_model_name = update_model_name(model)\n",
        "    global_output_dir = update_output_dir(output_dir)\n",
        "    print(f\"In get_question_and_answer - Current model: {global_model_name}\")\n",
        "    print(f\"In get_question_and_answer - Current output directory: {global_output_dir}\")\n",
        "    question = manual if predefined == \"manual input\" else predefined\n",
        "\n",
        "    if not question or not question.strip():\n",
        "        return \"Please ask a question.\", history, \"no valid question asked\"\n",
        "\n",
        "    print(f\"Processing question: {question}\")\n",
        "    print(f\"Using model: {model}\")\n",
        "\n",
        "    try:\n",
        "        # Pipe-Setup according to selected LLM\n",
        "        if \"claude\" in model.lower():\n",
        "            pipe = setup_claude_pipeline(model)\n",
        "        elif \"gpt\" in model.lower():\n",
        "            pipe = setup_openai_pipeline(model)\n",
        "        else:\n",
        "            pipe = setup_other_pipeline(model)\n",
        "\n",
        "        print(\"Pipeline setup complete\")\n",
        "\n",
        "        # detect the language of the answer - e.g. to respond in english if asked in english\n",
        "        question_language = detect_language(question)\n",
        "\n",
        "        # embeddings for question\n",
        "        embedder = BatchSentenceTransformersTextEmbedder()\n",
        "        question_embedding = embedder.run(text=question)[\"embedding\"]\n",
        "\n",
        "        print(\"Question embedding created\")\n",
        "\n",
        "        # retrieve relevant documents\n",
        "        retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=10)\n",
        "        relevant_docs = retriever.run(query_embedding=question_embedding)[\"documents\"]\n",
        "        print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
        "\n",
        "        # group documents according to source\n",
        "        grouped_documents = {}\n",
        "        for doc in relevant_docs:\n",
        "            source = doc.meta.get('source', 'Unknown')\n",
        "            if source not in grouped_documents:\n",
        "                grouped_documents[source] = []\n",
        "            grouped_documents[source].append(doc)\n",
        "\n",
        "        print(f\"Retrieved documents from: {list(grouped_documents.keys())}\")\n",
        "\n",
        "        result = pipe.run(\n",
        "            {\n",
        "                \"embedder\": {\"text\": question},\n",
        "                \"prompt_builder\": {\"question\": question, \"all_documents\": grouped_documents}\n",
        "            }\n",
        "        )\n",
        "        print(\"Pipeline run complete\")\n",
        "\n",
        "        # extract answer from selected LLM\n",
        "        if \"llm\" in result:\n",
        "            if \"replies\" in result[\"llm\"]:\n",
        "                # Handle Claude output\n",
        "                if isinstance(result[\"llm\"][\"replies\"][0], ChatMessage):\n",
        "                    answer = result[\"llm\"][\"replies\"][0].text\n",
        "                else:\n",
        "                    answer = result[\"llm\"][\"replies\"][0]\n",
        "            elif \"generated_texts\" in result[\"llm\"]:\n",
        "                answer = result[\"llm\"][\"generated_texts\"][0]\n",
        "            else:\n",
        "                raise ValueError(\"Unexpected response format from LLM\")\n",
        "        else:\n",
        "            raise ValueError(\"No LLM output found in pipeline result\")\n",
        "\n",
        "        print(f\"LLM Answer: {answer[:100]}...\")  # Print first 100 characters of the answer\n",
        "\n",
        "        # answer needs to be in string format\n",
        "        if not isinstance(answer, str):\n",
        "          answer = str(answer)\n",
        "\n",
        "        # clean answers if necessary\n",
        "        answer = answer.split(\"[Document:\")[0].strip()\n",
        "        if \"Question:\" in answer:\n",
        "            answer = answer.split(\"Question:\")[0].strip()\n",
        "\n",
        "        # Truncate the answer if it's too long (adjust the limit as needed)\n",
        "        max_tokens = 1000  # Adjust this value based on your needs\n",
        "        truncated_answer = truncate_text(answer, max_tokens)\n",
        "\n",
        "        if len(truncated_answer) < len(answer):\n",
        "            truncated_answer += \"\\n\\n[answers trunkated due to token restrictions.]\"\n",
        "\n",
        "        # Update history\n",
        "        history.append({\"question\": question, \"answer\": truncated_answer})\n",
        "        print(f\"Updated history length: {len(history)}\")\n",
        "\n",
        "        return truncated_answer, history, \"Question successfully answered\"\n",
        "\n",
        "        # Update history\n",
        "        history.append({\"question\": question, \"answer\": answer})\n",
        "        print(f\"Updated history length: {len(history)}\")\n",
        "\n",
        "        return answer, history, \"Question successfully answered\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_question_and_answer: {str(e)}\")\n",
        "        return f\"An error occurred: {str(e)}\", history, \"Error answering the question\"\n",
        "\n",
        "\n",
        "# update name of selected model for pdf report\n",
        "def update_model_name(name):\n",
        "    global global_model_name\n",
        "    global_model_name = name\n",
        "    print(f\"update_model_name called. Updated model name: {global_model_name}\")\n",
        "    return name\n",
        "\n",
        "# set directory to store pdf report\n",
        "def update_output_dir(dir):\n",
        "    global global_output_dir\n",
        "    global_output_dir = dir.strip() if dir and dir.strip() else None\n",
        "    print(f\"update_output_dir called. Updated output directory: {global_output_dir}\")\n",
        "    return dir\n",
        "\n",
        "# function called when files are removed from interface\n",
        "def on_files_removed(files):\n",
        "    if not files:  # If all files are removed\n",
        "        clear_document_store()\n",
        "        return \"document store emptied. You can upload new documents.\"\n",
        "    return \"Some documents have been removed. Please process pdfs again to update document store.\"\n",
        "\n",
        "# add FMA logo to report header\n",
        "def add_header(canvas, doc):\n",
        "    canvas.saveState()\n",
        "    # for the time being, logo has to be stored locally\n",
        "    logo_path = \"/content/florence 1.JPG\"\n",
        "\n",
        "    with PILImage.open(logo_path) as img:\n",
        "        img_width, img_height = img.size\n",
        "        img_mode = img.mode\n",
        "\n",
        "    aspect = img_height / float(img_width)\n",
        "\n",
        "    logo_width = doc.width\n",
        "    logo_height = logo_width * aspect\n",
        "\n",
        "    canvas.setFillColor(HexColor('#ec6600'))\n",
        "    canvas.rect(0, doc.height, doc.width + doc.leftMargin + doc.rightMargin, logo_height, fill=True)\n",
        "\n",
        "    canvas.drawImage(logo_path, doc.leftMargin, doc.height + doc.topMargin - logo_height,\n",
        "                     width=logo_width, height=logo_height, mask='auto' if img_mode == 'RGBA' else None)\n",
        "    canvas.restoreState()\n",
        "\n",
        "# functions for pdf report generation\n",
        "def generate_pdf_report_wrapper(history):\n",
        "    global global_model_name\n",
        "    global global_output_dir\n",
        "\n",
        "    try:\n",
        "        print(f\"generate_pdf_report_wrapper called.\")\n",
        "        print(f\"Current global_model_name: {global_model_name}\")\n",
        "        print(f\"Current global_output_dir: {global_output_dir}\")\n",
        "        print(f\"History length: {len(history)}\")\n",
        "        print(f\"History content: {history}\")\n",
        "\n",
        "        if not history:\n",
        "            return \"No questions and answers available to generate report.\"\n",
        "\n",
        "        model_name = global_model_name if global_model_name != \"Nicht spezifiziert\" else \"Standard-Modell\"\n",
        "        output_dir = global_output_dir if global_output_dir else os.getcwd()\n",
        "\n",
        "        return generate_pdf_report(history, llm_model=model_name, output_dir=output_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_pdf_report_wrapper: {str(e)}\")\n",
        "        return f\"Error in generate_pdf_report_wrapper: {str(e)}\"\n",
        "\n",
        "def generate_pdf_report(history, llm_model=None, output_dir=None):\n",
        "    print(f\"Received llm_model in generate_pdf_report: {llm_model}\")\n",
        "    print(f\"Type of llm_model: {type(llm_model)}\")\n",
        "    print(f\"Received output_dir in generate_pdf_report: {output_dir}\")\n",
        "    print(f\"Type of output_dir: {type(output_dir)}\")\n",
        "\n",
        "    if not history:\n",
        "        return \"No questions and answers available to generate report.\"\n",
        "\n",
        "    try:\n",
        "        # look after output directory\n",
        "        if output_dir is None or (isinstance(output_dir, str) and not output_dir.strip()):\n",
        "            output_dir = os.getcwd()\n",
        "\n",
        "        # output directory needs to be in string format\n",
        "        output_dir = str(output_dir)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        pdf_filename = f\"qa_report_{timestamp}.pdf\"\n",
        "        pdf_path = os.path.join(output_dir, pdf_filename)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        pdf_filename = f\"qa_report_{timestamp}.pdf\"\n",
        "        pdf_path = os.path.join(output_dir, pdf_filename)\n",
        "\n",
        "        # style the page of the generated report\n",
        "        doc = SimpleDocTemplate(pdf_path, pagesize=A4, topMargin=40*mm)\n",
        "        styles = getSampleStyleSheet()\n",
        "        styles.add(ParagraphStyle(name='Orange', textColor=HexColor('#ec6600')))\n",
        "        styles.add(ParagraphStyle(name='Small', fontSize=10, textColor=HexColor('#666666')))\n",
        "        story = []\n",
        "\n",
        "        page_template = PageTemplate(frames=[Frame(doc.leftMargin, doc.bottomMargin, doc.width, doc.height - 45*mm)],\n",
        "                                     onPage=add_header)\n",
        "        doc.addPageTemplates([page_template])\n",
        "\n",
        "        story.append(Paragraph(\"Q&A report\", styles['Title']))\n",
        "        story.append(Spacer(1, 6))\n",
        "\n",
        "        # store the name of the selected LLM\n",
        "        model_text = \"selected LLM model: \"\n",
        "        if llm_model and isinstance(llm_model, str) and llm_model.strip():\n",
        "            model_text += llm_model.strip()\n",
        "        else:\n",
        "            model_text += \"Nicht spezifiziert\"\n",
        "\n",
        "        print(f\"Model text to be added to PDF: {model_text}\")\n",
        "        story.append(Paragraph(model_text, styles['Small']))\n",
        "\n",
        "        story.append(Spacer(1, 12))\n",
        "\n",
        "        for i, qa in enumerate(history, 1):\n",
        "            question = qa.get('question', 'no question available')\n",
        "            answer = qa.get('answer', 'no answer available')\n",
        "\n",
        "            # Q&A need to be in string format\n",
        "            question = str(question)\n",
        "            answer = str(answer)\n",
        "\n",
        "            story.append(Paragraph(f\"Question {i}: {question}\", styles['Orange']))\n",
        "            story.append(Paragraph(answer, styles['BodyText']))\n",
        "            story.append(Spacer(1, 12))\n",
        "\n",
        "        doc.build(story)\n",
        "        return f\"pdf report generated: {pdf_path}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_pdf_report: {str(e)}\")\n",
        "        return f\"Error in generate_pdf_report: {str(e)}\"\n",
        "\n",
        "# Gradio interface\n",
        "# set theme for interface\n",
        "theme = gr.themes.Base(\n",
        "    primary_hue=\"gray\",\n",
        "    secondary_hue=\"orange\",\n",
        ").set(\n",
        "    # Button styles\n",
        "    button_primary_background_fill=\"*secondary_500\",\n",
        "    button_primary_background_fill_dark=\"*secondary_600\",\n",
        "    button_primary_background_fill_hover=\"*neutral_200\",  # Light gray on hover\n",
        "    button_primary_background_fill_hover_dark=\"*neutral_700\",  # Darker gray for dark mode\n",
        "    button_primary_border_color=\"*secondary_500\",\n",
        "    button_primary_border_color_hover=\"*neutral_200\",  # Light gray border on hover\n",
        "    button_primary_text_color=\"white\",\n",
        "    button_primary_text_color_hover=\"*neutral_800\",  # Dark text on light background\n",
        "    button_secondary_background_fill=\"*secondary_500\",\n",
        "    button_secondary_background_fill_dark=\"*secondary_600\",\n",
        "    button_secondary_background_fill_hover=\"*neutral_200\",  # Light gray on hover\n",
        "    button_secondary_background_fill_hover_dark=\"*neutral_700\",  # Darker gray for dark mode\n",
        "    button_secondary_border_color=\"*secondary_500\",\n",
        "    button_secondary_border_color_hover=\"*neutral_200\",  # Light gray border on hover\n",
        "    button_secondary_text_color=\"white\",\n",
        "    button_secondary_text_color_hover=\"*neutral_800\",  # Dark text on light background\n",
        "    # Slider styles\n",
        "    slider_color=\"*secondary_500\",\n",
        "    slider_color_dark=\"*secondary_600\",\n",
        ")\n",
        "\n",
        "# Custom CSS to style the warning text with higher specificity and !important\n",
        "custom_css = \"\"\"\n",
        ".warning-orange.custom-info {\n",
        "    margin-top: 0.5rem;\n",
        "    padding: 0.5rem;\n",
        "    border: 1px solid #ec6600;\n",
        "    border-radius: 4px;\n",
        "    background-color: #fff3e0;\n",
        "}\n",
        ".warning-orange.custom-info p {\n",
        "    color: #ec6600 !important;\n",
        "    font-weight: bold !important;\n",
        "    margin: 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# define parts of interface\n",
        "with gr.Blocks(theme=theme, css=custom_css) as iface:\n",
        "    history = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Image(\"/content/florence 1.JPG\", show_label=False, height=180, width=1500)\n",
        "\n",
        "    gr.Markdown(\"# ESGenius\")\n",
        "    gr.Markdown(\"Upload one or more pdf documents, preprocess them, select an LLM model and pose your questions.\")\n",
        "\n",
        "    with gr.Tab(\"process pdfs\"):\n",
        "        pdf_files = gr.Files(label=\"upload pdf documents\", file_types=[\".pdf\"])\n",
        "        split_by = gr.Dropdown(choices=[\"word\", \"sentence\", \"passage\"], value=\"word\", label=\"Split By\")\n",
        "        split_length = gr.Slider(minimum=100, maximum=1000, value=250, step=50, label=\"Split Length\", elem_classes=\"custom-slider\")\n",
        "        split_overlap = gr.Slider(minimum=0, maximum=500, value=50, step=50, label=\"Split Overlap\", elem_classes=\"custom-slider\")\n",
        "        process_button = gr.Button(\"process pdfs\")\n",
        "        pdf_output = gr.Textbox(label=\"process status\")\n",
        "        process_button.click(process_pdfs, inputs=[pdf_files, split_by, split_length, split_overlap], outputs=[pdf_output])\n",
        "\n",
        "        # event handler for removing files\n",
        "        pdf_files.change(on_files_removed, inputs=[pdf_files], outputs=[pdf_output])\n",
        "\n",
        "    with gr.Tab(\"Template Management\"):\n",
        "        gr.Markdown(\"Adapt the prompt template according to your needs! Changes will only take effect after clicking the button UPDATE TEMPLATE Buttons. Using the RESET TEMPLATE button, the original template can be restored. \")\n",
        "        template_text = gr.TextArea(\n",
        "            label=\"current template\",\n",
        "            value=current_template,\n",
        "            lines=20\n",
        "        )\n",
        "\n",
        "        warning_text = gr.Markdown(\n",
        "            \"⚠️ **Warning:** placeholders like {% for ... %}, {{ ... %}, must not be changed or deleted!\",\n",
        "            elem_classes=[\"warning-orange\", \"custom-info\"]\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            update_template_button = gr.Button(\"Update Template\", size=\"sm\")\n",
        "            reset_template_button = gr.Button(\"Reset Template\", size=\"sm\")\n",
        "\n",
        "        template_status = gr.Textbox(label=\"Template Status\")\n",
        "\n",
        "        update_template_button.click(\n",
        "            update_template_with_validation,\n",
        "            inputs=[template_text],\n",
        "            outputs=[template_status, template_text]\n",
        "        )\n",
        "\n",
        "        reset_template_button.click(\n",
        "            reset_template,\n",
        "            outputs=[template_text, template_status]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Q&A section\"):\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=[\n",
        "                \"HuggingFaceH4/zephyr-7b-beta\",\n",
        "                \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                \"claude-3-opus-20240229\",\n",
        "                \"claude-3-sonnet-20240229\",\n",
        "                \"claude-3-haiku-20240307\",\n",
        "                \"claude-3-7-sonnet-20250219\",\n",
        "                \"gpt-3.5-turbo\",\n",
        "                \"gpt-4\",\n",
        "                \"gpt-4-turbo-preview\"\n",
        "            ],\n",
        "            label=\"select LLM model\",\n",
        "            value=\"HuggingFaceH4/zephyr-7b-beta\"\n",
        "        )\n",
        "\n",
        "        predefined_question_dropdown = gr.Dropdown(\n",
        "            choices=predefined_questions,\n",
        "            label=\"choose a question or 'manual input'\",\n",
        "            value=\"manual input\"\n",
        "        )\n",
        "\n",
        "        manual_question_input = gr.Textbox(\n",
        "            lines=2,\n",
        "            placeholder=\"put your question here...\",\n",
        "            label=\"manual input\",\n",
        "            visible=True,\n",
        "            interactive=True\n",
        "        )\n",
        "\n",
        "\n",
        "        answer_button = gr.Button(\"answer question\")\n",
        "        answer_output = gr.Textbox(label=\"answer\")\n",
        "\n",
        "        output_dir_input = gr.Textbox(label=\"output directory for pdf (optional)\", placeholder=\"keep empty for current directory\")\n",
        "\n",
        "        generate_report_button = gr.Button(\"generate pdf report\")\n",
        "        report_status = gr.Textbox(label=\"status report\")\n",
        "\n",
        "\n",
        "    def toggle_manual_input(choice):\n",
        "        return gr.update(visible=(choice == \"manual input\"))\n",
        "\n",
        "    predefined_question_dropdown.change(\n",
        "        toggle_manual_input,\n",
        "        inputs=[predefined_question_dropdown],\n",
        "        outputs=[manual_question_input]\n",
        "    )\n",
        "\n",
        "    def clear_manual_input(choice):\n",
        "        return gr.update(visible=(choice == \"manual input\"), value=\"\")\n",
        "\n",
        "    def set_initial_state():\n",
        "        return gr.update(visible=True)\n",
        "\n",
        "    iface.load(set_initial_state, outputs=[manual_question_input])\n",
        "\n",
        "    predefined_question_dropdown.change(\n",
        "        clear_manual_input,\n",
        "        inputs=[predefined_question_dropdown],\n",
        "        outputs=[manual_question_input]\n",
        "    )\n",
        "\n",
        "    def update_model_name(name):\n",
        "        gr.State.model_name = name\n",
        "        return name\n",
        "\n",
        "    def update_output_dir(dir):\n",
        "        gr.State.output_dir = dir\n",
        "        return dir\n",
        "\n",
        "    def show_history(history):\n",
        "        return f\"current history: {history}\"\n",
        "\n",
        "    model_dropdown.change(update_model_name, inputs=[model_dropdown], outputs=[model_dropdown])\n",
        "    output_dir_input.change(update_output_dir, inputs=[output_dir_input], outputs=[output_dir_input])\n",
        "\n",
        "    answer_button.click(\n",
        "        get_question_and_answer,\n",
        "        inputs=[model_dropdown, predefined_question_dropdown, manual_question_input, history, output_dir_input],\n",
        "        outputs=[answer_output, history, report_status]\n",
        "    )\n",
        "\n",
        "    generate_report_button.click(\n",
        "        generate_pdf_report_wrapper,\n",
        "        inputs=[history],\n",
        "        outputs=[report_status]\n",
        "    )\n",
        "\n",
        "# start interface\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKG1q4TUp_U6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
